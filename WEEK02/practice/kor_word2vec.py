# 2. í•œêµ­ì–´ ì›Œë“œ ì„ë² ë”© êµ¬ì¶• ë° ì‹œê°í™”
# JDK 11 í•„ìš”

import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import jpype
from konlpy.tag import Okt
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from matplotlib import rc
import matplotlib.font_manager as fm
import os

# ì„¤ì¹˜ëœ í°íŠ¸ í™•ì¸
for f in fm.findSystemFonts(fontpaths=None, fontext='ttf'):
    if "Nanum" in f or "NotoSans" in f:
        rc('font', family='NanumBarunGothic')
        break


# 2-1. ë°ì´í„° ìˆ˜ì§‘
print("ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt", filename="practice/ratings_train.txt")
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="practice/ratings_test.txt")

# âœ… ë°ì´í„° ë¡œë“œ
train_dataset = pd.read_table("ratings_train.txt")
test_dataset = pd.read_table("ratings_test.txt")

# 2-2. ë°ì´í„° ì „ì²˜ë¦¬
print("ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
train_dataset.replace("", float("NaN"), inplace=True)
train_dataset = train_dataset.dropna().reset_index(drop=True)

# ì¤‘ë³µ ì œê±°
train_dataset = train_dataset.drop_duplicates(['document']).reset_index(drop=True)

# í•œê¸€ì´ ì•„ë‹Œ ë¬¸ì ì œê±°
train_dataset['document'] = train_dataset['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","", regex=True)

# ê¸¸ì´ê°€ ì§§ì€ ë°ì´í„° ì œê±°
train_dataset['document'] = train_dataset['document'].apply(
    lambda x: ' '.join([token for token in x.split() if len(token) > 2])
)

# ì „ì²´ ê¸¸ì´ê°€ 10 ì´í•˜ì´ê±°ë‚˜ ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ê°€ 5ê°œ ì´í•˜ì¸ ë°ì´í„° ì œê±°
train_dataset = train_dataset[train_dataset.document.apply(
    lambda x: len(str(x)) > 10 and len(str(x).split()) > 5
)].reset_index(drop=True)

# ë¶ˆìš©ì–´ ì •ì˜
stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼',
             'ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']

# 2-2. í† í°í™”
print("í˜•íƒœì†Œ ë¶„ì„ê¸° í† í°í™” ì¤‘...")

okt = Okt()
tokenized_data = []

for sentence in train_dataset['document']:
    tokenized_sentence = okt.morphs(sentence, stem=True)   # í† í°í™”
    stopwords_removed = [word for word in tokenized_sentence if word not in stopwords]  # ë¶ˆìš©ì–´ ì œê±°
    tokenized_data.append(stopwords_removed)

# 2-3. ë°ì´í„° ë¶„í¬ í™•ì¸
print("ë°ì´í„° ë¶„í¬ í™•ì¸ ì¤‘...")

print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ :', max(len(review) for review in tokenized_data))
print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :', sum(map(len, tokenized_data))/len(tokenized_data))

plt.hist([len(review) for review in tokenized_data], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

# 2-4. ì›Œë“œ ì„ë² ë”© êµ¬ì¶•
print("ì›Œë“œ ì„ë² ë”© êµ¬ì¶• ì¤‘...")

embedding_dim = 100

model = Word2Vec(
    sentences=tokenized_data,
    vector_size=embedding_dim,
    window=5,
    min_count=5,
    workers=4,
    sg=0  # CBOW
)

word_vectors = model.wv
vocabs = list(word_vectors.key_to_index.keys())

# ìœ ì‚¬ë„ í™•ì¸
print("ğŸ‘‰ 'ë§ˆë¸”'ê³¼ ìœ ì‚¬í•œ ë‹¨ì–´:")
for sim_word in model.wv.most_similar("ë§ˆë¸”"):
    print(sim_word)

print("ğŸ‘‰ 'ìŠ¬í””' vs 'ëˆˆë¬¼' ìœ ì‚¬ë„:", model.wv.similarity('ìŠ¬í””', 'ëˆˆë¬¼'))

# 2-5. PCAë¥¼ ì´ìš©í•œ ì‹œê°í™”
print("PCA ì‹œê°í™” ì¤‘...")

word_vector_list = [word_vectors[word] for word in vocabs]

pca = PCA(n_components=2)
xys = pca.fit_transform(word_vector_list)

x_axis = xys[:, 0]
y_axis = xys[:, 1]

def plot_pca_graph(vocabs, x_axis, y_axis):
    plt.figure(figsize=(25, 15))
    plt.scatter(x_axis, y_axis, marker='o')
    for i, v in enumerate(vocabs):
        plt.annotate(v, xy=(x_axis[i], y_axis[i]))
    plt.show()

plot_pca_graph(vocabs, x_axis, y_axis)

# 2-6. t-SNEë¥¼ ì´ìš©í•œ ì‹œê°í™”
print("t-SNE ì‹œê°í™” ì¤‘...")

tsne = TSNE(learning_rate=100, n_iter=1000, perplexity=30)
word_vector_list = np.array(word_vector_list)
transformed = tsne.fit_transform(word_vector_list)

x_axis_tsne = transformed[:, 0]
y_axis_tsne = transformed[:, 1]

def plot_tsne_graph(vocabs, x_axis, y_axis):
    plt.figure(figsize=(30, 30))
    plt.scatter(x_axis, y_axis, marker='o')
    for i, v in enumerate(vocabs):
        plt.annotate(v, xy=(x_axis[i], y_axis[i]))
    plt.show()

plot_tsne_graph(vocabs, x_axis_tsne, y_axis_tsne)

# 2-7. ì„ë² ë”© í”„ë¡œì í„°ìš© ì €ì¥

print("ì„ë² ë”© í”„ë¡œì í„°ìš© íŒŒì¼ ì €ì¥ ì¤‘...")

model.wv.save_word2vec_format('sample_word2vec_embedding')

# gensim ë‚´ì¥ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰í•˜ë©´ í…ì„œë³´ë“œì—ì„œ Embedding Projectorë¡œ í™•ì¸ ê°€ëŠ¥
# !python -m gensim.scripts.word2vec2tensor --input sample_word2vec_embedding --output sample_word2vec_embedding
